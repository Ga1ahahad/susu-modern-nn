{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy  as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "GDKEx0oD3E_p"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OPlDvmDWzEzr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = tf.keras.utils.normalize(X_train, axis=1)\n",
        "X_test = tf.keras.utils.normalize(X_test, axis=1)\n",
        "y_train = y_train[:, np.newaxis].T\n",
        "y_test = y_test[:, np.newaxis].T\n",
        "\n",
        "x = X_train\n",
        "xo = np.ndarray((len(x),28**2))\n",
        "for i in range(len(x)):\n",
        "  xo[i] = x[i].flatten()\n",
        "X_train = xo.T\n",
        "\n",
        "x = X_test\n",
        "xo = np.ndarray((len(x),28**2))\n",
        "for i in range(len(x)):\n",
        "  xo[i] = x[i].flatten()\n",
        "X_test = xo.T"
      ],
      "metadata": {
        "id": "eZIuZ0eizFZg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def onehot(Y):\n",
        "  y_new = []\n",
        "  for y in Y[0]:\n",
        "    t = [0,0,0,0,0,0,0,0,0,0]\n",
        "    t[y] = 1\n",
        "    y_new.append(t)\n",
        "  return np.array(y_new).T"
      ],
      "metadata": {
        "id": "yzjQxpEzjy1o"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reverse_onehot(Y):\n",
        "  y = Y.T\n",
        "  out = []\n",
        "  for yi in y:\n",
        "    out.append(np.argmax(yi))\n",
        "  return np.array(out).T"
      ],
      "metadata": {
        "id": "dy0QOXMi3TLu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = onehot(y_train)\n",
        "y_test = onehot(y_test)"
      ],
      "metadata": {
        "id": "VQXBquqpmJTd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dlnet:\n",
        "    def __init__(self, x, y):\n",
        "        self.X=x  # входные данные\n",
        "        self.Y=y  # размеченные данные, target\n",
        "        self.Yh=np.zeros((1,self.Y.shape[1]))  # реальный выход перцептрона\n",
        "        self.L=2  # количество слоев\n",
        "        self.dims = [28*28, 20, 10]  # 28**2 фичей на вход, 20 нейронов в скрытом слое,\n",
        "                                # 10 нейронов на выходе\n",
        "        self.param = {}  # параметры и базисы для каждого слоя\n",
        "        self.ch = {}  # кэш для вякого\n",
        "        self.grad = {}  #\n",
        "        self.loss = []  # для хранения значений лоссов\n",
        "        self.lr=0.003\n",
        "        self.sam = self.Y.shape[1]  # количество тренировачных образцов\n",
        "\n",
        "    def nInit(self):\n",
        "        \"\"\"\n",
        "        Инициализирует начальные параметры для сети\n",
        "        \"\"\"\n",
        "        np.random.seed(1)\n",
        "\n",
        "        self.param['W1'] = np.random.randn(self.dims[1], self.dims[0]) / np.sqrt(self.dims[0])\n",
        "        self.param['b1'] = np.zeros((self.dims[1], 1))\n",
        "        self.param['W2'] = np.random.randn(self.dims[2], self.dims[1]) / np.sqrt(self.dims[1])\n",
        "        self.param['b2'] = np.zeros((self.dims[2], 1))\n",
        "        return\n",
        "\n",
        "    def Sigmoid(self, Z):\n",
        "        return 1/(1+np.exp(-Z))\n",
        "\n",
        "    def Relu(self, Z):\n",
        "        return np.maximum(0,Z)\n",
        "\n",
        "    def forward(self):\n",
        "        # первый слой\n",
        "        Z1 = self.param['W1'].dot(self.X) + self.param['b1']\n",
        "        # функция активации\n",
        "        A1 = self.Relu(Z1)\n",
        "        # сохраняем в кэш результаты\n",
        "        self.ch['Z1'],self.ch['A1']=Z1,A1\n",
        "        # второй слой\n",
        "        Z2 = self.param['W2'].dot(A1) + self.param['b2']\n",
        "        # функция активации\n",
        "        A2 = self.Sigmoid(Z2)\n",
        "        # сохраняем в кэш результаты\n",
        "        self.ch['Z2'],self.ch['A2']=Z2,A2\n",
        "        # выход сети\n",
        "        self.Yh=A2\n",
        "        # считаем лоссы\n",
        "        loss=self.nloss(A2)\n",
        "        #print(self.param)\n",
        "        #print(self.Yh)\n",
        "        return self.Yh, loss\n",
        "\n",
        "    def nloss(self,Yh):\n",
        "        # кросс-энтропия\n",
        "        loss = (1./self.sam) * (-np.dot(self.Y,np.log(Yh).T) - np.dot(1-self.Y, np.log(1-Yh).T))\n",
        "        return loss\n",
        "\n",
        "    def dRelu(self, x):\n",
        "        x[x<=0] = 0\n",
        "        x[x>0] = 1\n",
        "        return x\n",
        "\n",
        "    def dSigmoid(self, Z):\n",
        "        s = 1/(1+np.exp(-Z))\n",
        "        dZ = s * (1-s)\n",
        "        return dZ\n",
        "\n",
        "    def backward(self):\n",
        "        dLoss_Yh = - (np.divide(self.Y, self.Yh ) - np.divide(1 - self.Y, 1 - self.Yh))\n",
        "\n",
        "        dLoss_Z2 = dLoss_Yh * self.dSigmoid(self.ch['Z2'])\n",
        "        dLoss_W2 = 1./self.ch['A1'].shape[1] * np.dot(dLoss_Z2,self.ch['A1'].T)\n",
        "        dLoss_b2 = 1./self.ch['A1'].shape[1] * np.dot(dLoss_Z2, np.ones([dLoss_Z2.shape[1],1]))\n",
        "        #dLoss_W2 = np.dot(dLoss_Z2,self.ch['A1'].T)\n",
        "        #dLoss_b2 = np.dot(dLoss_Z2, np.ones([dLoss_Z2.shape[1],1]))\n",
        "\n",
        "        dLoss_A1 = np.dot(self.param[\"W2\"].T,dLoss_Z2)\n",
        "        dLoss_Z1 = dLoss_A1 * self.dRelu(self.ch['Z1'])\n",
        "        dLoss_A0 = np.dot(self.param[\"W1\"].T,dLoss_Z1)\n",
        "        dLoss_W1 = 1./self.X.shape[1] * np.dot(dLoss_Z1,self.X.T)\n",
        "        dLoss_b1 = 1./self.X.shape[1] * np.dot(dLoss_Z1, np.ones([dLoss_Z1.shape[1],1]))\n",
        "        #dLoss_W1 = np.dot(dLoss_Z1,self.X.T)\n",
        "        #dLoss_b1 = np.dot(dLoss_Z1, np.ones([dLoss_Z1.shape[1],1]))\n",
        "\n",
        "        self.param[\"W1\"] = self.param[\"W1\"] - self.lr * dLoss_W1\n",
        "        self.param[\"b1\"] = self.param[\"b1\"] - self.lr * dLoss_b1\n",
        "        self.param[\"W2\"] = self.param[\"W2\"] - self.lr * dLoss_W2\n",
        "        self.param[\"b2\"] = self.param[\"b2\"] - self.lr * dLoss_b2\n",
        "\n",
        "    def gd(self,X, Y, iter = 3000):\n",
        "        np.random.seed(10)\n",
        "\n",
        "        self.nInit()\n",
        "\n",
        "        for i in range(0, iter):\n",
        "            Yh, loss=self.forward()\n",
        "            self.backward()\n",
        "\n",
        "            if i % 1 == 0:\n",
        "                print (\"Cost after iteration %i: %f\" %(i, loss.sum()))\n",
        "                self.loss.append(loss)\n",
        "\n",
        "        return\n",
        "\n",
        "    def pred(self,x, y):\n",
        "        self.X=x\n",
        "        self.Y=y\n",
        "        comp = np.zeros((10,x.shape[1]))\n",
        "        pred, loss= self.forward()\n",
        "        acc = 0\n",
        "\n",
        "        for i in range(0, pred.shape[1]):\n",
        "          #print(pred[:,i])\n",
        "          comp[np.argmax(pred[:,i]),i] = 1\n",
        "          acc = acc + np.dot(comp[:,i], y[:,i])\n",
        "\n",
        "        print(acc/x.shape[1])\n",
        "        return comp"
      ],
      "metadata": {
        "id": "x7xfSIIS5LDJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "am = 10000\n",
        "a = X_train[:,0:am]\n",
        "b = y_train[:,0:am]"
      ],
      "metadata": {
        "id": "3gpda_tMCthu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = dlnet(a, b)\n",
        "nn.lr=0.2\n",
        "nn.dims = [28**2, 20, 10]\n",
        "nn.gd(X_train, y_train, iter = 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgc1Af4_AfIe",
        "outputId": "7aed9c9c-2891-48ec-f289-7a227b8a8ac8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 68.440585\n",
            "Cost after iteration 1: 58.274883\n",
            "Cost after iteration 2: 44.529184\n",
            "Cost after iteration 3: 34.987634\n",
            "Cost after iteration 4: 33.194717\n",
            "Cost after iteration 5: 33.042171\n",
            "Cost after iteration 6: 33.046202\n",
            "Cost after iteration 7: 33.095489\n",
            "Cost after iteration 8: 33.167841\n",
            "Cost after iteration 9: 33.253009\n",
            "Cost after iteration 10: 33.347900\n",
            "Cost after iteration 11: 33.450292\n",
            "Cost after iteration 12: 33.558432\n",
            "Cost after iteration 13: 33.671695\n",
            "Cost after iteration 14: 33.789146\n",
            "Cost after iteration 15: 33.911227\n",
            "Cost after iteration 16: 34.037467\n",
            "Cost after iteration 17: 34.168095\n",
            "Cost after iteration 18: 34.302855\n",
            "Cost after iteration 19: 34.441670\n",
            "Cost after iteration 20: 34.584982\n",
            "Cost after iteration 21: 34.733147\n",
            "Cost after iteration 22: 34.886033\n",
            "Cost after iteration 23: 35.043617\n",
            "Cost after iteration 24: 35.206149\n",
            "Cost after iteration 25: 35.373644\n",
            "Cost after iteration 26: 35.546277\n",
            "Cost after iteration 27: 35.724432\n",
            "Cost after iteration 28: 35.907643\n",
            "Cost after iteration 29: 36.096599\n",
            "Cost after iteration 30: 36.291440\n",
            "Cost after iteration 31: 36.492364\n",
            "Cost after iteration 32: 36.699772\n",
            "Cost after iteration 33: 36.913680\n",
            "Cost after iteration 34: 37.134519\n",
            "Cost after iteration 35: 37.361999\n",
            "Cost after iteration 36: 37.596634\n",
            "Cost after iteration 37: 37.838241\n",
            "Cost after iteration 38: 38.086937\n",
            "Cost after iteration 39: 38.342271\n",
            "Cost after iteration 40: 38.604423\n",
            "Cost after iteration 41: 38.873089\n",
            "Cost after iteration 42: 39.148085\n",
            "Cost after iteration 43: 39.428746\n",
            "Cost after iteration 44: 39.714397\n",
            "Cost after iteration 45: 40.005225\n",
            "Cost after iteration 46: 40.300737\n",
            "Cost after iteration 47: 40.600333\n",
            "Cost after iteration 48: 40.902975\n",
            "Cost after iteration 49: 41.208110\n",
            "Cost after iteration 50: 41.515963\n",
            "Cost after iteration 51: 41.825979\n",
            "Cost after iteration 52: 42.137195\n",
            "Cost after iteration 53: 42.449350\n",
            "Cost after iteration 54: 42.761859\n",
            "Cost after iteration 55: 43.075009\n",
            "Cost after iteration 56: 43.388411\n",
            "Cost after iteration 57: 43.701711\n",
            "Cost after iteration 58: 44.014607\n",
            "Cost after iteration 59: 44.327070\n",
            "Cost after iteration 60: 44.638951\n",
            "Cost after iteration 61: 44.950350\n",
            "Cost after iteration 62: 45.260720\n",
            "Cost after iteration 63: 45.569803\n",
            "Cost after iteration 64: 45.877616\n",
            "Cost after iteration 65: 46.183950\n",
            "Cost after iteration 66: 46.488635\n",
            "Cost after iteration 67: 46.791901\n",
            "Cost after iteration 68: 47.093525\n",
            "Cost after iteration 69: 47.393297\n",
            "Cost after iteration 70: 47.691424\n",
            "Cost after iteration 71: 47.987584\n",
            "Cost after iteration 72: 48.281968\n",
            "Cost after iteration 73: 48.574332\n",
            "Cost after iteration 74: 48.864530\n",
            "Cost after iteration 75: 49.152825\n",
            "Cost after iteration 76: 49.439093\n",
            "Cost after iteration 77: 49.723273\n",
            "Cost after iteration 78: 50.005502\n",
            "Cost after iteration 79: 50.285640\n",
            "Cost after iteration 80: 50.563849\n",
            "Cost after iteration 81: 50.839888\n",
            "Cost after iteration 82: 51.113763\n",
            "Cost after iteration 83: 51.385604\n",
            "Cost after iteration 84: 51.655289\n",
            "Cost after iteration 85: 51.922921\n",
            "Cost after iteration 86: 52.188457\n",
            "Cost after iteration 87: 52.451866\n",
            "Cost after iteration 88: 52.713472\n",
            "Cost after iteration 89: 52.972968\n",
            "Cost after iteration 90: 53.230493\n",
            "Cost after iteration 91: 53.486006\n",
            "Cost after iteration 92: 53.739623\n",
            "Cost after iteration 93: 53.991209\n",
            "Cost after iteration 94: 54.240793\n",
            "Cost after iteration 95: 54.488606\n",
            "Cost after iteration 96: 54.734359\n",
            "Cost after iteration 97: 54.978138\n",
            "Cost after iteration 98: 55.219812\n",
            "Cost after iteration 99: 55.459535\n",
            "Cost after iteration 100: 55.697204\n",
            "Cost after iteration 101: 55.932874\n",
            "Cost after iteration 102: 56.166892\n",
            "Cost after iteration 103: 56.398888\n",
            "Cost after iteration 104: 56.629139\n",
            "Cost after iteration 105: 56.857479\n",
            "Cost after iteration 106: 57.083843\n",
            "Cost after iteration 107: 57.308408\n",
            "Cost after iteration 108: 57.531341\n",
            "Cost after iteration 109: 57.752485\n",
            "Cost after iteration 110: 57.971648\n",
            "Cost after iteration 111: 58.189069\n",
            "Cost after iteration 112: 58.404852\n",
            "Cost after iteration 113: 58.618728\n",
            "Cost after iteration 114: 58.830798\n",
            "Cost after iteration 115: 59.041239\n",
            "Cost after iteration 116: 59.250154\n",
            "Cost after iteration 117: 59.457337\n",
            "Cost after iteration 118: 59.662854\n",
            "Cost after iteration 119: 59.866791\n",
            "Cost after iteration 120: 60.069003\n",
            "Cost after iteration 121: 60.269771\n",
            "Cost after iteration 122: 60.469002\n",
            "Cost after iteration 123: 60.666665\n",
            "Cost after iteration 124: 60.862971\n",
            "Cost after iteration 125: 61.057666\n",
            "Cost after iteration 126: 61.251166\n",
            "Cost after iteration 127: 61.443052\n",
            "Cost after iteration 128: 61.633564\n",
            "Cost after iteration 129: 61.822559\n",
            "Cost after iteration 130: 62.010165\n",
            "Cost after iteration 131: 62.196358\n",
            "Cost after iteration 132: 62.381671\n",
            "Cost after iteration 133: 62.565876\n",
            "Cost after iteration 134: 62.748905\n",
            "Cost after iteration 135: 62.930638\n",
            "Cost after iteration 136: 63.111226\n",
            "Cost after iteration 137: 63.290472\n",
            "Cost after iteration 138: 63.468439\n",
            "Cost after iteration 139: 63.645219\n",
            "Cost after iteration 140: 63.820936\n",
            "Cost after iteration 141: 63.995504\n",
            "Cost after iteration 142: 64.168893\n",
            "Cost after iteration 143: 64.341084\n",
            "Cost after iteration 144: 64.512007\n",
            "Cost after iteration 145: 64.682284\n",
            "Cost after iteration 146: 64.851500\n",
            "Cost after iteration 147: 65.019588\n",
            "Cost after iteration 148: 65.186750\n",
            "Cost after iteration 149: 65.352716\n",
            "Cost after iteration 150: 65.518007\n",
            "Cost after iteration 151: 65.682079\n",
            "Cost after iteration 152: 65.845267\n",
            "Cost after iteration 153: 66.007311\n",
            "Cost after iteration 154: 66.168186\n",
            "Cost after iteration 155: 66.328088\n",
            "Cost after iteration 156: 66.486987\n",
            "Cost after iteration 157: 66.645337\n",
            "Cost after iteration 158: 66.802619\n",
            "Cost after iteration 159: 66.959070\n",
            "Cost after iteration 160: 67.114472\n",
            "Cost after iteration 161: 67.269200\n",
            "Cost after iteration 162: 67.422950\n",
            "Cost after iteration 163: 67.575334\n",
            "Cost after iteration 164: 67.726502\n",
            "Cost after iteration 165: 67.876639\n",
            "Cost after iteration 166: 68.026180\n",
            "Cost after iteration 167: 68.174565\n",
            "Cost after iteration 168: 68.322140\n",
            "Cost after iteration 169: 68.468704\n",
            "Cost after iteration 170: 68.613757\n",
            "Cost after iteration 171: 68.757655\n",
            "Cost after iteration 172: 68.900987\n",
            "Cost after iteration 173: 69.043426\n",
            "Cost after iteration 174: 69.185127\n",
            "Cost after iteration 175: 69.326036\n",
            "Cost after iteration 176: 69.466365\n",
            "Cost after iteration 177: 69.606023\n",
            "Cost after iteration 178: 69.744836\n",
            "Cost after iteration 179: 69.882762\n",
            "Cost after iteration 180: 70.019779\n",
            "Cost after iteration 181: 70.156147\n",
            "Cost after iteration 182: 70.291452\n",
            "Cost after iteration 183: 70.425416\n",
            "Cost after iteration 184: 70.558425\n",
            "Cost after iteration 185: 70.690442\n",
            "Cost after iteration 186: 70.821760\n",
            "Cost after iteration 187: 70.952568\n",
            "Cost after iteration 188: 71.082626\n",
            "Cost after iteration 189: 71.211997\n",
            "Cost after iteration 190: 71.340664\n",
            "Cost after iteration 191: 71.468716\n",
            "Cost after iteration 192: 71.596165\n",
            "Cost after iteration 193: 71.722865\n",
            "Cost after iteration 194: 71.848927\n",
            "Cost after iteration 195: 71.974478\n",
            "Cost after iteration 196: 72.098934\n",
            "Cost after iteration 197: 72.222772\n",
            "Cost after iteration 198: 72.345860\n",
            "Cost after iteration 199: 72.468210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = nn.pred(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwzVFTieA150",
        "outputId": "4e60ba44-91b0-4e5a-ef4c-587904b55a45"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cn = reverse_onehot(c)\n",
        "yn = reverse_onehot(y_test)\n",
        "(cn==yn).sum()"
      ],
      "metadata": {
        "id": "MhIFxEAn27fX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c6d2db7-95aa-4bf7-e22d-9784765ce8c1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8817"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.param"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi7kx_8c34rX",
        "outputId": "dcae7f88-d6f7-473c-ca4a-8c7aea5c0ae7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W1': array([[-0.00048843, -0.08006767, -0.07387169, ..., -0.09314973,\n",
              "         -0.06869802, -0.04037303],\n",
              "        [ 0.04976308, -0.06305008, -0.02254226, ...,  0.0136139 ,\n",
              "         -0.044953  ,  0.02865558],\n",
              "        [-0.0071055 , -0.08399323, -0.0955485 , ..., -0.08248019,\n",
              "         -0.01026263, -0.0177748 ],\n",
              "        ...,\n",
              "        [-0.06192667, -0.03115272, -0.01610096, ..., -0.01465761,\n",
              "          0.02152715, -0.03931117],\n",
              "        [ 0.08412721,  0.0580769 ,  0.01999432, ..., -0.02181765,\n",
              "          0.0573722 ,  0.0167679 ],\n",
              "        [ 0.03637285, -0.02879049,  0.01252853, ...,  0.03474712,\n",
              "         -0.02558065, -0.03943756]]),\n",
              " 'b1': array([[ 0.30964715],\n",
              "        [-0.03315856],\n",
              "        [ 0.09431504],\n",
              "        [ 0.08110452],\n",
              "        [-0.01121579],\n",
              "        [ 0.15386934],\n",
              "        [-0.01437769],\n",
              "        [ 0.06722785],\n",
              "        [ 0.1960012 ],\n",
              "        [ 0.23346187],\n",
              "        [-0.03908138],\n",
              "        [ 0.09218334],\n",
              "        [ 0.22362162],\n",
              "        [ 0.20658485],\n",
              "        [-0.05602078],\n",
              "        [ 0.00842001],\n",
              "        [-0.02502355],\n",
              "        [ 0.14059715],\n",
              "        [-0.02895987],\n",
              "        [ 0.06462132]]),\n",
              " 'W2': array([[ 0.28136494,  0.37423734,  0.07522459, -0.56006935,  0.02674922,\n",
              "         -0.01129386,  0.09081367, -0.45491413,  0.59464965, -0.17341403,\n",
              "          0.32667903, -0.08789758,  0.06187317, -1.31786874,  0.48690974,\n",
              "         -0.12343326,  0.05620025, -1.45619164,  0.1200392 , -0.32414648],\n",
              "        [-1.35279212, -0.27305707,  0.26175521, -0.73842874,  0.10499332,\n",
              "          0.25393406, -0.2025472 ,  0.55941274, -1.20508755, -0.32520211,\n",
              "          0.28686577, -0.00455362,  0.04925558,  0.55052402, -0.14270318,\n",
              "         -0.02095824, -0.24238905,  0.11930205,  0.03275485,  0.37110452],\n",
              "        [-0.86008783, -0.04186282, -0.44918119, -0.77629413,  0.07104048,\n",
              "         -0.73804285,  0.17362146, -0.50947003,  0.14543467,  0.77046064,\n",
              "         -0.23607335,  0.19701708, -0.37326743,  0.51673592, -0.14817423,\n",
              "          0.06987444, -0.27866556, -0.34889977, -0.07748743, -0.69066947],\n",
              "        [ 0.20432215,  0.11384145, -0.80577671,  0.4384154 ,  0.25486842,\n",
              "         -0.48802287, -0.04261172, -0.38209108, -1.52650319,  0.8011272 ,\n",
              "         -0.21031848,  0.15646181, -0.10239944, -0.43982124, -0.28857576,\n",
              "          0.01147112,  0.42357844, -0.17782033,  0.13924549, -0.26809424],\n",
              "        [ 0.03948004,  0.11635011, -1.15194465, -0.328977  ,  0.06789075,\n",
              "          0.0162565 , -0.14611134, -0.64015889, -0.23360856, -1.52405976,\n",
              "         -0.087835  , -0.36013763,  0.33099985, -0.0363126 ,  0.16357893,\n",
              "         -0.27160241, -0.22915151,  0.85541539,  0.40290198, -0.03941966],\n",
              "        [-0.11852863,  0.33112361,  0.09324672, -0.09788777, -0.16881533,\n",
              "          1.0971262 ,  0.22950252,  0.18528094, -0.63390184, -0.30221181,\n",
              "          0.4039783 ,  0.22322937,  0.2654127 , -1.37092319,  0.19661543,\n",
              "          0.38233357,  0.13950053, -0.12860681, -0.17723249, -0.06362514],\n",
              "        [-1.51034511,  0.34053139, -0.31205787, -0.2429651 ,  0.10988238,\n",
              "          0.07246784,  0.28388937, -0.48260113,  1.01544703, -0.23539399,\n",
              "         -0.09332059, -0.30841351,  0.00748082, -0.93946609, -0.17429841,\n",
              "          0.0522577 ,  0.36644442,  0.67144224,  0.02476597, -0.13405527],\n",
              "        [-0.3320997 , -0.03683883, -0.16615691,  0.75239204,  0.04743672,\n",
              "         -0.9978486 , -0.18073171,  0.62233395, -0.01864642, -0.63105743,\n",
              "         -0.1174259 , -0.32744889,  0.61878964,  0.19178993, -0.0276804 ,\n",
              "          0.03182849, -0.1156231 , -1.34172081,  0.00476144, -0.00425131],\n",
              "        [-0.29062209,  0.27243603,  0.98141754, -0.0720504 , -0.08518228,\n",
              "         -0.30398701, -0.17516275,  0.04420576, -0.37463999,  0.00845089,\n",
              "          0.36225793, -0.30698698, -0.99483503, -0.70834458,  0.24294671,\n",
              "          0.13619762, -0.02710492,  0.26171628, -0.07890066,  0.02046054],\n",
              "        [-0.20318331,  0.07662346,  0.0072281 ,  0.81858601,  0.01423956,\n",
              "         -0.49685437, -0.22209424, -0.53493473, -0.10906913, -1.13572918,\n",
              "         -0.15597698,  0.33813527, -0.71788238,  0.11067462,  0.16164987,\n",
              "         -0.23559611, -0.04792959,  0.00293207,  0.07260224, -0.11088559]]),\n",
              " 'b2': array([[-0.35667231],\n",
              "        [-0.37223778],\n",
              "        [-0.14834013],\n",
              "        [-0.23513203],\n",
              "        [-0.24419126],\n",
              "        [-0.40872619],\n",
              "        [-0.58295584],\n",
              "        [-0.3734317 ],\n",
              "        [-0.31655937],\n",
              "        [-0.39129835]])}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OeqvB6JL4_VW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}